{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error might be better than mean absolute error\n",
    "\n",
    "-> Because squared error allows gradient descent to converge in a better way. More explanation will be provided later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and cost function\n",
    "\n",
    "As every single entry from our dataset is fed in the neural network, error (consider absolute errors) is calculated for each. Then we sum up all the errors and take its mean. Individual errors is called loss and the mean is called cost function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
