{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error might be better than mean absolute error\n",
    "\n",
    "-> Because squared error allows gradient descent to converge in a better way. More explanation will be provided later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and cost function\n",
    "\n",
    "As every single training sample is fed in the neural network, error (consider absolute errors) is calculated for each. Then we sum up all the errors and take its mean. Individual errors is called loss and the mean is called cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch\n",
    "\n",
    "After we go through all the training samples once, its called an epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss / Binary Cross Entropy\n",
    "\n",
    "-> Read its formula once. It is used heavily in logistic regression, infact no other loss functions are used much.\n",
    "\n",
    "The reason for that is given in the following link -> https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
