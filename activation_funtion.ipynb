{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use Activation Function?\n",
    "\n",
    "They provide non-linearity to the weighted sums of inputs.\n",
    "\n",
    "## Why do we need non-linearity?\n",
    "\n",
    "If we remove all the activation functions from all the hidden layer and input output layer neurons, we get simple linear equations which do not even need any hidden layers. And as we already know that linear equations or simple line cannot fit every kind of pattern that this universe has to offer, that's why we need non-linearity in complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "## Step Function -> \n",
    "\n",
    "Useful for binary classification, but for multi-class classification, will give 0 and 1 to various classes, which will become difficult to manage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function ->\n",
    "\n",
    "In case of multi-class classification, gives probabilities for various classes to happen, and we choose class with highest prob. Better than step, since provides a smooth curve between data points rather than a simple step line. Compresses output values between 0 & 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh function\n",
    "\n",
    "Like sigmoid, but compresses output values around 0, and in (-1 to 1) range which is better than sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common understanding is that use sigmoid activation in output layer, and tanh anywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues with Sigmoid and tanh functions\n",
    "\n",
    "Since the curves in both the functions flatten at certain point, gradient for the curve starts becoming 0. Calculating gradients is necessary for backpropagation and error calculation, which in \"0 gradient\" case becomes difficult and so, the learning process for the algorithm halts/becomes slow. This is called the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU function\n",
    "\n",
    "which returns 0 for negative input and the value itself for positive value. This is a computationally cheap function. The ReLU function also has vanishing gradient problem, since negative inputs result in a constant line which has a slope of 0. \n",
    "\n",
    "### When in doubt, use ReLU function by default for hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU function\n",
    "\n",
    "This function kind of solves the problem of vanishing gradient by giving a small weight to the output for a negative input, so there is 0 slope line, and the function looks like -> {ax, x}, ax for negative and x for positive x."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
